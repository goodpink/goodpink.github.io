<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Webpage of Ryo Yonetani</title>
    <link>https://yonetaniryo.github.io/ja/</link>
      <atom:link href="https://yonetaniryo.github.io/ja/index.xml" rel="self" type="application/rss+xml" />
    <description>Webpage of Ryo Yonetani</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>ja</language>
    <image>
      <url>https://yonetaniryo.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Webpage of Ryo Yonetani</title>
      <link>https://yonetaniryo.github.io/ja/</link>
    </image>
    
    <item>
      <title>Profile</title>
      <link>https://yonetaniryo.github.io/ja/profile_j/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://yonetaniryo.github.io/ja/profile_j/</guid>
      <description>&lt;img class=&#34;img-circle img-responsive&#34; src=&#34;https://yonetaniryo.github.io/images/me.png&#34; style=&#39;width:30%&#39;&gt;
&lt;h1 id=&#34;米谷竜よねたにりょう&#34;&gt;米谷　竜（よねたに　りょう）&lt;/h1&gt;
&lt;p&gt;情報科学分野において，主にコンピュータビジョンと機械学習の研究に従事しています．
京都大学で博士号取得後，東大生産技術研究所助教，カーネギーメロン大学訪問研究員を経て，現在オムロンサイニックエックス株式会社でPIとして働いています．&lt;/p&gt;
&lt;h2 id=&#34;学歴&#34;&gt;学歴&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;博士（情報学）&lt;/li&gt;
&lt;li&gt;H25.11 京都大学　大学院情報学研究科　知能情報学専攻　博士課程　期間短縮修了
&lt;ul&gt;
&lt;li&gt;&amp;ldquo;Modeling Spatiotemporal Correlations between Video Saliency and Gaze Dynamics&amp;rdquo;（主査: 松山隆司先生）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;H23.3 京都大学　大学院情報学研究科　知能情報学専攻　修士課程　修了
&lt;ul&gt;
&lt;li&gt;「映像の顕著性変動と視線運動の時空間相関分析に基づいた集中度推定」（主査: 松山隆司先生）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;H21.3 京都大学　電気電子工学科　卒業
&lt;ul&gt;
&lt;li&gt;「提示イベントと眼球動作との同期構造分析に基づく注視対象推定」（指導教員: 松山隆司先生）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;職位&#34;&gt;職位&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;R2-現在 オムロンサイニックエックス株式会社 PI&lt;/li&gt;
&lt;li&gt;H31-現在 東京大学生産技術研究所 協力研究員&lt;/li&gt;
&lt;li&gt;H31-R2 オムロンサイニックエックス株式会社 シニアリサーチャー&lt;/li&gt;
&lt;li&gt;H26-H30 東京大学生産技術研究所 助教&lt;/li&gt;
&lt;li&gt;H30     オムロンサイニックエックス株式会社 技術アドバイザ&lt;/li&gt;
&lt;li&gt;H28-H29 Visiting Scholar at Carnegie Mellon University, Robotics Institute&lt;/li&gt;
&lt;li&gt;H24-H26 日本学術振興会特別研究員（DC2/PD）&lt;/li&gt;
&lt;li&gt;H23-H24 京都大学グローバルCOEリサーチアシスタント&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;受賞&#34;&gt;受賞&lt;/h2&gt;
&lt;h4 id=&#34;本人によるもの&#34;&gt;本人によるもの&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;MIRUインタラクティブ賞, 画像の認識・理解シンポジウム, 2019&lt;/li&gt;
&lt;li&gt;CVPR2017 Outstanding Reviewer, IEEE Conference on Computer Vision and Pattern Recognition, 2017&lt;/li&gt;
&lt;li&gt;山下記念研究賞, 情報処理学会, 2016&lt;/li&gt;
&lt;li&gt;情報・システムソサイエティ論文賞, 電子情報通信学会, 2014&lt;/li&gt;
&lt;li&gt;H25 PRMU研究奨励賞, 電子情報通信学会パターン認識とメディア理解研究会, 2014&lt;/li&gt;
&lt;li&gt;MIRU優秀学生論文賞, 画像の認識・理解シンポジウム, 2012&lt;/li&gt;
&lt;li&gt;IBM Best Student Paper Award (Track IV: Biometrics and Human Computer Interaction), International Conference on Pattern Recognition, 2010&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;共著学生によるもの&#34;&gt;共著学生によるもの&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;杉田祐樹, インタラクティブ発表賞, インタラクション2017, 2017&lt;/li&gt;
&lt;li&gt;村上晋太郎, CVIM奨励賞, 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016&lt;/li&gt;
&lt;li&gt;樋口未来, CVIM奨励賞, 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;研究助成&#34;&gt;研究助成&lt;/h2&gt;
&lt;h4 id=&#34;代表者&#34;&gt;代表者&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JST戦略的創造研究推進事業（ACT-I）, &amp;ldquo;プライバシー保護一人称ビジョン&amp;rdquo;, 2016 - 2018&lt;/li&gt;
&lt;li&gt;生産技術研究奨励会特定研究奨励助成, &amp;ldquo;一人称視点映像の共通注目シーン解析&amp;rdquo;, 2016 - 2017&lt;/li&gt;
&lt;li&gt;科学研究費補助金若手研究B, &amp;ldquo;映像コンテンツの顕著性変動解析による特徴ベース視線推定&amp;rdquo;, 2015 - 2017&lt;/li&gt;
&lt;li&gt;栢森情報科学振興財団助成, &amp;ldquo;複数装着型カメラ映像からのインタラクション認識技術の開発&amp;rdquo;, 2015 - 2017&lt;/li&gt;
&lt;li&gt;大川情報通信基金研究助成, &amp;ldquo;ディスプレイを高原として用いた視線計測技術に関する研究&amp;rdquo;, 2015 - 2016&lt;/li&gt;
&lt;li&gt;日本学術振興会特別研究員研究助成, &amp;ldquo;時区間ハイブリッドダイナミカルシステムを用いた心の分析とモデル化&amp;rdquo;, 2012-2014&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;共同研究者-研究協力者&#34;&gt;共同研究者, 研究協力者&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;JST戦略的創造研究推進事業（CREST）, &amp;ldquo;集合視による注視・行動解析に基づくライフイノベーション創出&amp;rdquo;（代表: 佐藤洋一）, 2014 - 現在&lt;/li&gt;
&lt;li&gt;JST戦略的国際共同研究プログラム（SICORP）, &amp;ldquo;多様なカメラを活用した群衆行動の変化検出&amp;rdquo;（代表: 佐藤洋一）, 2016 - 現在&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;学会活動&#34;&gt;学会活動&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Publicity Chair @ &lt;a href=&#34;http://accv2020.kyoto/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Asian Conference on Computer Vision (ACCV2020)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;画像の認識・理解シンポジウム (MIRU) 2019 財務委員長&lt;/li&gt;
&lt;li&gt;Program Committee Member @ &lt;a href=&#34;https://cvcops19.cispa.saarland/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security (CV-COPS2019)&lt;/a&gt; at CVPR2019&lt;/li&gt;
&lt;li&gt;Organizer @ &lt;a href=&#34;http://www.sys.info.hiroshima-cu.ac.jp/aiu2018/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Workshop on Attention/Intention Understanding (AIU2018)&lt;/a&gt; at ACCV2018.&lt;/li&gt;
&lt;li&gt;情報処理学会コンピュータビジョンとイメージメディア（CVIM）研究会 幹事, 2018-現在&lt;/li&gt;
&lt;li&gt;Program Committee Member @ &lt;a href=&#34;http://vision.soic.indiana.edu/bright-and-dark-workshop-2018/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security (CV-COPS2018)&lt;/a&gt; at CVPR2018&lt;/li&gt;
&lt;li&gt;情報処理学会 論文誌ジャーナル/JIP編集委員会 編集委員, 2017-現在&lt;/li&gt;
&lt;li&gt;Program Chair @ &lt;a href=&#34;http://printeps.org/HDC2017/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;International Workshop on Human Activity Analysis with Highly Diverse Cameras (HDC2017)&lt;/a&gt; at WACV2017&lt;/li&gt;
&lt;li&gt;Organizing Committee Member @ IAPR International Conference on Machine Vision and Applications (MVA2017)&lt;/li&gt;
&lt;li&gt;Sponsorship Chair @ International Conference on Multimodal Interaction (ICMI2016)&lt;/li&gt;
&lt;li&gt;情報処理学会コンピュータビジョンとイメージメディア（CVIM）研究会 運営委員, 2014 - 2018&lt;/li&gt;
&lt;li&gt;Reviewer @ CVPR (2015 - present), ECCV (2014 - present), ICCV (2015 - present), ACCV (2014 - present), CHI (2017), ICPR (2014), TPAMI, IJCV, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;招待講演講義&#34;&gt;招待講演，講義&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;サーベイ論文の書き方 ～視覚的注意モデルのサーベイを題材に～, パターン認識とメディア理解研究会, 2018（&lt;a href=&#34;https://www.dropbox.com/s/vvqxs698en01uf2/PRMU180518_yonetani_small.pdf?dl=0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;資料（10.8MB）&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;一人称ビジョンと集合視, 【第22回AIセミナー】 「コンピュータービジョンとAI　～人とロボットの視覚～」, 産総研 人工知能研究センター, 2018（&lt;a href=&#34;https://goo.gl/SW3kbS&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;資料 (146MB)&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;グランドチャレンジは誰がやるのか, 特別セッション「今後の研究会のあり方を考える」, パターン認識とメディア理解研究会, 2018（&lt;a href=&#34;https://goo.gl/nz4eMf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;資料&lt;/a&gt;）&lt;/li&gt;
&lt;li&gt;一人称ビジョン, 人工知能とメディア, はこだて未来大学, 2017&lt;/li&gt;
&lt;li&gt;一人称視点映像解析の最先端, 画像の認識・理解シンポジウム, 2016&lt;/li&gt;
&lt;li&gt;Recognizing Micro-Actions and Reactions from Paired Egocentric Videos, 画像の認識・理解シンポジウム, 2016&lt;/li&gt;
&lt;li&gt;Ego-Surfing First-Person Videos, 画像の認識・理解シンポジウム, 2015&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;発表論文&#34;&gt;発表論文&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;http://dblp.uni-trier.de/pers/hd/y/Yonetani:Ryo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DBLP&lt;/a&gt; | &lt;a href=&#34;https://scholar.google.com/citations?user=DYXnRWEAAAAJ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Scholar&lt;/a&gt; | &lt;a href=&#34;http://ci.nii.ac.jp/nrid/9000017546008&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CiNii&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;査読付き雑誌論文&#34;&gt;査読付き雑誌論文&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ryo Yonetani, Kris Kitani, Yoichi Sato: &amp;ldquo;Ego-Surfing: Person Localization in First-Person Videos Using Ego-Motion Signatures&amp;rdquo;, IEEE Transactions on Pattern Analysis and Machine Intelligence &lt;strong&gt;(TPAMI)&lt;/strong&gt;, Vol.40, Issue 11, pp.2749-2761, 2018.&lt;/li&gt;
&lt;li&gt;下西慶, 石川惠理奈, 米谷竜, 川嶋宏彰, 松山隆司: &amp;ldquo;視線運動解析による興味アスペクトの推定&amp;rdquo;, ヒューマンインタフェース学会論文誌, Vol.16, No2, pp.103-114, 2014&lt;/li&gt;
&lt;li&gt;Akisato Kimura, Ryo Yonetani, Takatsugu Hirayama: &amp;ldquo;Computational Models of Human Visual Attention and Their Implementations: A Survey&amp;rdquo;, IEICE Transactions on Information and Systems, E96-D(3), pp.562-578, 2013&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: Learning Spatiotemporal Gaps between Where We Look and What We Focus on&amp;quot;, IPSJ Transactions on Computer Vision and Applications, 5, pp. 75-79, 2013&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 加藤丈和, 松山隆司: &amp;ldquo;映像の顕著性変動モデルを用いた視聴者の集中状態推定, 電子情報通信学会論文誌, J96-D(8), pp.1675-1687, 2013（第15回 画像の認識・理解シンポジウム推薦論文）&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, Takatsugu Hirayama, Takashi Matsuyama: &amp;ldquo;Mental Focus Analysis Using the Spatio-temporal Correlation between Visual Saliency and Eye Movements&amp;rdquo;, 情報処理学会論文誌, Vol. 52, No. 12, 2011&lt;/li&gt;
&lt;li&gt;石川惠理奈, 米谷竜, 平山高嗣, 松山隆司: &amp;ldquo;Gaze Mirroringによる注視模倣効果の分析&amp;rdquo;, 情報処理学会論文誌, Vol.52, No.12, pp3637-3646, 2011&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: &amp;ldquo;Gaze Probing: イベント提示に基づく注視オブジェクト推定&amp;rdquo;, ヒューマンインタフェース学会論文誌, Vol.12, No.3, pp. 125-135, 2010&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;国際会議論文&#34;&gt;国際会議論文&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Mai Nishimura, Ryo Yonetani, &amp;ldquo;L2B: Learning to Balance the Safety-Efficiency Trade-off in Interactive Crowd-aware Robot Navigation&amp;rdquo;, accepted to IEEE/RSJ International Conference on Intelligent Robots and Systems &lt;strong&gt;(IROS)&lt;/strong&gt;, 2020, &lt;a href=&#34;https://arxiv.org/abs/2003.09207&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mohammadamin Barekatain, Ryo Yonetani, Masashi Hamaya, &amp;ldquo;MULTIPOLAR: Multi-Source Policy Aggregation for Transfer Reinforcement Learning between Diverse Environmental Dynamics&amp;rdquo;, accepted to International Joint Conference on Artificial Intelligence &lt;strong&gt;(IJCAI)&lt;/strong&gt;, 2020 [&lt;a href=&#34;https://arxiv.org/abs/1909.13111&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;arXiv&lt;/a&gt;]&lt;/li&gt;
&lt;li&gt;Rie Kamikubo, Naoya Kato, Keita Higuchi, Ryo Yonetani, Yoichi Sato, &amp;ldquo;Studying Effective Agents in Remote Sighted Guidance for People Navigating with Visual Impairments&amp;rdquo;, ACM Conference on Human Factors in Computing Systems &lt;strong&gt;(CHI)&lt;/strong&gt;, 2020&lt;/li&gt;
&lt;li&gt;Navyata Sanghvi, Ryo Yonetani, Kris Kitani, &amp;ldquo;Modeling Social Group Communication with Multi-Agent Imitation Learning&amp;rdquo;, International Conference on Autonomous Agents and Multi-Agent Systems &lt;strong&gt;(AAMAS)&lt;/strong&gt;, 2020 &lt;a href=&#34;https://arxiv.org/abs/1903.01537&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Naoya Yoshida, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto, Ryo Yonetani, &amp;ldquo;Hybrid-FL for Wireless Networks: Cooperative Learning Mechanism Using Non-IID Data&amp;rdquo;, IEEE International Conference on Communications &lt;strong&gt;(ICC)&lt;/strong&gt;, 2020 &lt;a href=&#34;https://arxiv.org/abs/1905.07210&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Takayuki Nishio and Ryo Yonetani: &amp;ldquo;Client Selection for Federated Learning with Heterogeneous Resources in Mobile Edge&amp;rdquo;, IEEE International Conference on Communications &lt;strong&gt;(ICC)&lt;/strong&gt;, 2019 &lt;a href=&#34;https://yonetaniryo.github.io/2018/04/24/ny-arxiv2018.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Nathawan Charoenkulvanich, Rie Kamikubo, Ryo Yonetani, and Yoichi Sato, &amp;ldquo;Assisting Group Activity Analysis through Hand Detection and Identification in Multiple Egocentric Videos&amp;rdquo;, ACM Conference on Intelligent User Interface &lt;strong&gt;(IUI)&lt;/strong&gt;, 2019.&lt;/li&gt;
&lt;li&gt;Yuki Sugita, Keita Higuchi, Ryo Yonetani, Rie Kamikubo, Yoichi Sato: &amp;ldquo;Browsing Group First-Person Videos with 3D Visualization&amp;rdquo;, accepted to ACM International Conference on Interactive Surfaces and Spaces (ISS), 2018&lt;/li&gt;
&lt;li&gt;Takuma Yagi, Karttikeya Mangalam, Ryo Yonetani, Yoichi Sato: &amp;ldquo;Future Person Localization in First-Person Videos&amp;rdquo;, IEEE Conference on Computer Vision and Pattern Recognition &lt;strong&gt;(CVPR, spotlight presentation)&lt;/strong&gt;, 2018 &lt;a href=&#34;https://yonetaniryo.github.io/2018/02/19/ymys-cvpr2018.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Rie Kamikubo, Keita Higuchi, Ryo Yonetani, Hideki Koike, Yoichi Sato, &amp;ldquo;Exploring the Role of Tunnel Vision Simulation in the Design Cycle of Accessible Interfaces&amp;rdquo;, International Cross-Disciplinary Conference on Web Accessibility &lt;strong&gt;(Web4All)&lt;/strong&gt;, 2018&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Vishnu Naresh Boddeti, Kris M. Kitani, Yoichi Sato: &amp;ldquo;Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption&amp;rdquo;, International Conference on Computer Vision &lt;strong&gt;(ICCV)&lt;/strong&gt;, 2017 &lt;a href=&#34;https://yonetaniryo.github.io/2017/07/16/ybks-iccv2017.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Keita Higuchi, Ryo Yonetani, Yoichi Sato: &amp;ldquo;EgoScanning: Quickly Scanning First-Person Videos with Egocentric Elastic Timelines&amp;rdquo;, ACM Conference on Human Factors in Computing Systems &lt;strong&gt;(CHI)&lt;/strong&gt;, 2017 &lt;a href=&#34;https://yonetaniryo.github.io/2017/01/16/hys-chi2017.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Kris Kitani, Yoichi Sato: &amp;ldquo;Visual Motif Discovery via First-Person Vision&amp;rdquo;, European Conference on Computer Vision &lt;strong&gt;(ECCV)&lt;/strong&gt;, 2016 &lt;a href=&#34;https://yonetaniryo.github.io/2016/07/12/yks-eccv2016.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Kris Kitani, Yoichi Sato: &amp;ldquo;Recognizing Micro-Actions and Reactions from Paired Egocentric Videos&amp;rdquo;, IEEE Conference on Computer Vision and Pattern Recognition &lt;strong&gt;(CVPR)&lt;/strong&gt;, 2016 &lt;a href=&#34;https://yonetaniryo.github.io/2016/03/02/yks-cvpr2016.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Keita Higuchi, Ryo Yonetani, Yoichi Sato: &amp;ldquo;Can Eye Help You?: Effects of Visualizing Eye Fixations on Remote Collaboration Scenarios for Physical Tasks&amp;rdquo;, ACM Conference on Human Factors in Computing Systems &lt;strong&gt;(CHI)&lt;/strong&gt;, 2016 &lt;a href=&#34;https://yonetaniryo.github.io/2016/01/18/hys-chi2016.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Kris Kitani, Yoichi Sato: &amp;ldquo;Ego-Surfing First-Person Videos&amp;rdquo;, IEEE Conference on Computer Vision and Pattern Recognition &lt;strong&gt;(CVPR)&lt;/strong&gt;, 2015 &lt;a href=&#34;https://yonetaniryo.github.io/2016/06/15/yks-cvpr2015.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: &amp;ldquo;Predicting Where We Look from Spatiotemporal Gaps&amp;rdquo;, International Conference on Multimodal Interaction &lt;strong&gt;(ICMI)&lt;/strong&gt;, 2013 &lt;a href=&#34;https://yonetaniryo.github.io/2013/12/09/ykm-icmi2013.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Akisato Kimura, Hitoshi Sakano, Ken Fukuchi: &amp;ldquo;Single Image Segmentation with Estimated Depth&amp;rdquo;, British Machine Vision Conference &lt;strong&gt;(BMVC)&lt;/strong&gt;, 2012&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: &amp;ldquo;Multi-mode Saliency Dynamics Model for Analyzing Gaze and Attention&amp;rdquo;, Eye Tracking Research &amp;amp; Applications &lt;strong&gt;(ETRA)&lt;/strong&gt;, 2012 &lt;a href=&#34;https://yonetaniryo.github.io/2012/03/28/ykm-etra2012.html&#34;&gt;[project]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, Takatsugu Hirayama, Takashi Matsuyama: &amp;ldquo;Gaze Probing: Event-Based Estimation of Objects Being Focused On&amp;rdquo;, International Conference on Pattern Recognition &lt;strong&gt;(ICPR, IBM Best Student Paper Award)&lt;/strong&gt;, 2010&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;国際ワークショップ論文extended-abstracts-プレプリントなど&#34;&gt;国際ワークショップ論文，Extended Abstracts, プレプリントなど&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Ryo Yonetani, Tatsunori Taniai, Mohammadamin Barekatain, Mai Nishimura, Asako Kanezaki, &amp;ldquo;Path Planning using Neural A* Search&amp;rdquo;, arXiv preprint, 2020 &lt;a href=&#34;https://arxiv.org/abs/2009.07476&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jiaxin Ma, Ryo Yonetani, Zahid Iqbal, &amp;ldquo;Adaptive Distillation for Decentralized Learning from Heterogeneous Clients&amp;rdquo;, arXiv preprint, 2020 &lt;a href=&#34;https://arxiv.org/abs/2008.07948&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Tomohiro Takahashi, Atsushi Hashimoto, Yoshitaka Ushiku, &amp;ldquo;Decentralized Learning of Generative Adversarial Networks from Multi-Client Non-iid Data&amp;rdquo;, arXiv preprint, 2019 &lt;a href=&#34;https://arxiv.org/abs/1905.09684&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Navyata Sanghvi, Ryo Yonetani, Kris Kitani, &amp;ldquo;Modeling Social Group Communication with Multi-Agent Imitation Learning&amp;rdquo;, arXiv preprint, 2019 &lt;a href=&#34;https://arxiv.org/abs/1903.01537&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[arXiv]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Navyata Sanghvi, Ryo Yonetani, Kris Kitani, &amp;ldquo;Learning Group Communication from Demonstration&amp;rdquo;, RSS Workshop on Models and Representations for Natural Human-Robot Communication, 2018&lt;/li&gt;
&lt;li&gt;Seita Kayukawa, Keita Higuchi, Ryo Yonetani, Maanori Nakamura, Yoichi Sato, Shigeo Morishima: &amp;ldquo;Dynamic Object Scanning: Object-Based Elastic Timeline for Quickly Browsing First-Person Videos&amp;rdquo;, ACM Conference on Human Factors in Computing Systems Late Breaking Work &lt;strong&gt;(CHI-LBW)&lt;/strong&gt;, 2018&lt;/li&gt;
&lt;li&gt;Keita Higuchi, Ryo Yonetani, Yoichi Sato: &amp;ldquo;EgoScanning: Quickly Scanning First-Person Videos with Egocentric Elastic Timelines&amp;rdquo;, ACM SIGGRAPH Asia Emerging Technologies &lt;strong&gt;(SIGGRAPH-ASIA-ETECH)&lt;/strong&gt;, 2017&lt;/li&gt;
&lt;li&gt;Yifei Huang, Minjie Cai, Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: &amp;ldquo;Temporal Localization and Spatial Segmentation of Joint Attention in Multiple First-Person Video&amp;rdquo;, International Workshop on Egocentric Perception, Interaction, and Computing &lt;strong&gt;(EPIC)&lt;/strong&gt;, 2017&lt;/li&gt;
&lt;li&gt;Rie Kamikubo, Keita Higuchi, Ryo Yonetani, Hideki Koike, Yoichi Sato: &amp;ldquo;Rapid Prototyping of Accessible Interfaces with Gaze- Contiguent Tunnel Vision Simulation&amp;rdquo;, ACM SIGACCESS International Conference on Computers and Accessibility &lt;strong&gt;(ASSETS)&lt;/strong&gt;, 2017&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Vishnu Naresh Boddeti, Kris Kitani, Yoichi Sato: &amp;ldquo;Privacy-Preserving Visual Learning Using Doubly Permuted Homomorphic Encryption&amp;rdquo;, International Workshop on The Bright and Dark Sides of Computer Vision: Challenges and Opportunities for Privacy and Security &lt;strong&gt;(CV-COPS)&lt;/strong&gt;, 2017&lt;/li&gt;
&lt;li&gt;Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: &amp;ldquo;Discovering Objects of Joint Attention via First-Person Sensing&amp;rdquo;, IEEE CVPR Workshop on Egocentric (First-Person) Vision &lt;strong&gt;(EGOV)&lt;/strong&gt;, 2016&lt;/li&gt;
&lt;li&gt;Kei Shimonishi, Hiroaki Kawashima, Ryo Yonetani, Erina Ishikawa, Takashi Matsuyama: &amp;ldquo;Learning Aspects of Interest from Gaze&amp;rdquo;, ICMI Workshop on Eye Gaze in Intelligent Human Machine Interaction: Gaze in Multimodal Interaction &lt;strong&gt;(GazeIn)&lt;/strong&gt;, 2013&lt;/li&gt;
&lt;li&gt;Ryo Yonetani: &amp;ldquo;Modeling Video Viewing Behaviors for Viewer State Estimation&amp;rdquo;, ACM Multimedia Doctoral Symposium &lt;strong&gt;(ACMMM-DS)&lt;/strong&gt;, 2012&lt;/li&gt;
&lt;li&gt;Erina Ishikawa, Ryo Yonetani, Hiroaki Kawashima, Takatsugu Hirayama, Takashi Matsuyama: &amp;ldquo;Semantic Interpretation of Eye Movements Using Designed Structures of Displayed Contents&amp;rdquo;, ICMI Workshop on Eye Gaze in Intelligent Human Machine Interaction: Eye Gaze, Multimodality &lt;strong&gt;(GazeIn)&lt;/strong&gt;, 2012&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;国内会議論文&#34;&gt;国内会議論文&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;八木 拓真, マンガラムカーティケヤ, 米谷 竜, 佐藤 洋一, &amp;ldquo;一人称視点映像における人物位置予測&amp;rdquo;, 情報処理学会研究会資料 CVIM, 2018&lt;/li&gt;
&lt;li&gt;粥川青汰, 樋口啓太, 中村優文, 米谷竜, 佐藤洋一, 森島繁生, ”一人称視点動画の高速閲覧に有効なキューの自動生成手法”, インタラクティブシステムとソフトウェアに関するワークショップ,  2017&lt;/li&gt;
&lt;li&gt;粥川青汰, 樋口啓太, 中村優文, 米谷竜, 佐藤洋一, 森島繁生, ”物体検出とユーザ入力に基づく一人称視点映像の高速閲覧手法”, 情報処理学会コンピュータビジョンとイメージメディア研究会,  2017.&lt;/li&gt;
&lt;li&gt;Yifei Huang, Minjie Cai, Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: &amp;ldquo;Spatial-temporal Segmentation of Joint Attention in Multiple First-Person Videos&amp;rdquo;, 第20回 画像の認識理解シンポジウム, 2017&lt;/li&gt;
&lt;li&gt;杉田祐樹, 樋口啓太, 米谷竜, 佐藤洋一: &amp;ldquo;複数一人称視点映像閲覧における行動空間とカメラ位置姿勢の3次元可視化による効果&amp;rdquo;, 情報処理学会シンポジウム　インタラクション2017, 2017**（インタラクティブ発表賞）**&lt;/li&gt;
&lt;li&gt;中野雄介, 米谷竜, 樋口啓太, 佐藤洋一: &amp;ldquo;視線を考慮した一人称視点映像からの頷き検出&amp;rdquo;, 電子情報通信学会総合大会, 2017&lt;/li&gt;
&lt;li&gt;杉田祐樹, 樋口啓太, 米谷竜, 佐藤洋一: &amp;ldquo;複数一人称視点映像閲覧における行動空間とカメラ位置姿勢の3次元可視化による効果&amp;rdquo;, 情報処理学会ヒューマンコンピュータインタラクション研究会, 2017&lt;/li&gt;
&lt;li&gt;樋口啓太, 米谷竜, 佐藤洋一: &amp;ldquo;伸縮タイムライン生成による一人称視点映像の高速閲覧支援&amp;rdquo;, 第24回インタラクティブシステムとソフトウェアに関するワークショップ, 2016&lt;/li&gt;
&lt;li&gt;樋口未来, 米谷竜, 木谷クリス, 佐藤洋一: &amp;ldquo;一人称視点映像を用いたランキング学習による相対的地位の推定&amp;rdquo;, 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016**（情報処理学会CVIM研究会奨励賞）**&lt;/li&gt;
&lt;li&gt;Hiroshi Kera, Ryo Yonetani, Keita Higuchi, Yoichi Sato: &amp;ldquo;Discovering Objects of Joint Attention via First-Person Sensing&amp;rdquo;, 第19回 画像の認識理解シンポジウム,2016&lt;/li&gt;
&lt;li&gt;樋口啓太, 米谷竜, 佐藤洋一: &amp;ldquo;手の動作に基づく複数一人称視点作業映像のアラインメント&amp;rdquo;, 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016&lt;/li&gt;
&lt;li&gt;松本大輝, 米谷竜, 佐藤洋一: &amp;ldquo;滑動性眼球運動を用いた視線計測の自動校正&amp;rdquo;, 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016&lt;/li&gt;
&lt;li&gt;村上晋太郎, 米谷竜, 佐藤洋一: &amp;ldquo;視線を利用した二人称視点動作認識&amp;rdquo;, 情報処理学会コンピュータビジョンとイメージメディア研究会, 2016**（情報処理学会CVIM研究会奨励賞）**&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Kris Kitani, Yoichi Sato: &amp;ldquo;Ego-Surfing First-Person Videos&amp;rdquo;, 第18回 画像の認識理解シンポジウム, 2015&lt;/li&gt;
&lt;li&gt;松本 大輝, 米谷 竜, 佐藤 洋一, &amp;ldquo;滑動性眼球運動を用いた視線計測の自動校正&amp;rdquo;, 第18回 画像の認識理解シンポジウム, 2015&lt;/li&gt;
&lt;li&gt;杉田 祐樹, 米谷 竜, 佐藤 洋一, &amp;ldquo;一人称視点映像における視線情報を活用した自己アクティビティ認識&amp;rdquo;, 第18回 画像の認識理解シンポジウム, 2015&lt;/li&gt;
&lt;li&gt;樋口 未来, 木谷 クリス 真実, 米谷 竜, 佐藤 洋一, &amp;ldquo;一人称視点映像を用いた話者間の相対的地位の推定&amp;rdquo;, 第18回 画像の認識理解シンポジウム, 2015&lt;/li&gt;
&lt;li&gt;樋口啓太, 米谷竜, 佐藤洋一: &amp;ldquo;遠隔作業支援シナリオにおける注視位置可視化の効果&amp;rdquo;, 第23回インタラクティブシステムとソフトウェアに関するワークショップ, 2015&lt;/li&gt;
&lt;li&gt;神窪利絵, 樋口啓太, 米谷竜, 小池英樹, 佐藤洋一: &amp;ldquo;弱視者のための視線計測を用いたウェブアクセシビリティの向上&amp;rdquo;, 電子情報通信学会福祉情報工学研究会, 信学技報WIT2015-75, 2015&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, and Takashi Matsuyama: Modeling Spatiotemporal Correlations between Video Saliency and Gaze Dynamics&amp;rdquo;, 研究報告コンピュータビジョンとイメージメディア, 2014-CVIM-192(32), pp. 1-16, 2014&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 松山隆司: &amp;ldquo;映像閲覧行動の時空間ずれ構造モデルを用いた注視点予測&amp;rdquo;, 信学技報, vol. 113, no. 196, PRMU2013-41, pp. 57-62, 2013**（2013年度PRMU研究奨励賞）**&lt;/li&gt;
&lt;li&gt;Ryo Yonetani, Hiroaki Kawashima, Takashi Matsuyama: &amp;ldquo;Learning Spatiotemporal Gaps between Where We Look and What We Focus on&amp;rdquo;, 第16回 画像の認識理解シンポジウム, 2013&lt;/li&gt;
&lt;li&gt;下西慶, 川嶋宏彰, 米谷竜, 松山隆司: &amp;ldquo;視線運動解析による興味アスペクトの推定&amp;rdquo;, 信学技報, vol. 113, no. 75, PRMU2013-28, pp. 53-58, 2013&lt;/li&gt;
&lt;li&gt;石川惠理奈, 米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: &amp;ldquo;提示コンテンツのデザイン構造を用いた視線運動の意味理解&amp;rdquo;, 電子情報通信学会技術報告, PRMU2012-60, vol. 112, no. 225, pp.47-52, 2012&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 加藤丈和, 松山隆司: &amp;ldquo;映像の顕著性変動モデルを用いた視聴者の集中状態推定&amp;rdquo;, 第15回 画像の認識・理解シンポジウム, 2012**（MIRU優秀学生論文賞）**&lt;/li&gt;
&lt;li&gt;石川惠理奈, 米谷竜, 平山高嗣, 松山隆司: &amp;ldquo;Gaze Mirroringによる注視模倣効果の分析&amp;rdquo;, ヒューマンインタフェースシンポジウム2011, pp.561-566, 2011&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: &amp;ldquo;映像の顕著性変動と視線運動の時空間相関分析に基づいた集中状態推定&amp;rdquo;, 情報処理学会研究会資料, CVIM178-16, 2011&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: &amp;ldquo;注視オブジェクト推定のための動的コンテンツデザインとその評価&amp;rdquo;, 情報処理学会創立50周年記念（第72回）全国大会, 32N-1, pp. 5-141-142, 2010&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: &amp;ldquo;Gaze Probing, &amp;ldquo;イベント提示に基づく注視対象推定&amp;rdquo;, 第12回画像の認識・理解シンポジウム, pp.1713-1720, 2009&lt;/li&gt;
&lt;li&gt;米谷竜, 川嶋宏彰, 平山高嗣, 松山隆司: &amp;ldquo;提示イベントと眼球動作との同期構造分析に基づく注視対象推定&amp;rdquo;, 情報処理学会研究会資料 CVIM 167-16, 2009&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;特許&#34;&gt;特許&lt;/h4&gt;
&lt;p&gt;&lt;a href=&#34;https://patents.google.com/?inventor=Ryo&amp;#43;Yonetani&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://patents.google.com/?inventor=Ryo+Yonetani&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
